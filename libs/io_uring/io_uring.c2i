module io_uring;

import c2 local;
import uio;

import csignal;
import linux_types;
import sys_socket;

// ------ liburing.h ------
/*
 * Library interface to io_uring
 */

type Sq struct @(cname="io_uring_sq") {
    u32* khead;
    u32* ktail;
    u32* kring_mask;
    u32* kring_entries;
    u32* kflags;
    u32* kdropped;
    u32* array;
    Sqe* sqes;

    u32 sqe_head;
    u32 sqe_tail;

    usize ring_sz;
    void* ring_ptr;

    u32[4] pad;
}
static_assert(104, sizeof(Sq));


type Cq struct @(cname="io_uring_cq") {
    u32* khead;
    u32* ktail;
    u32* kring_mask;
    u32* kring_entries;
    u32* kflags;
    u32* koverflow;
    Cqe* cqes;

    usize ring_sz;
    void* ring_ptr;

    u32[4] pad;
}
static_assert(88, sizeof(Cq));


type Ring struct @(cname="io_uring") {
    Sq sq;
    Cq cq;
    u32 flags;
    c_int ring_fd;

    u32 features;
    u32[3] pad;
}
static_assert(216, sizeof(Ring));


/*
 * Library interface
 */

#if 0
/*
 * return an allocated io_uring_probe structure, or NULL if probe fails (for
 * example, if it is not available). The caller is responsible for freeing it
 */
struct io_uring_probe *io_uring_get_probe_ring(struct io_uring *ring);
/* same as io_uring_get_probe_ring, but takes care of ring init and teardown */
struct io_uring_probe *io_uring_get_probe(void);

/*
 * frees a probe allocated through io_uring_get_probe() or
 * io_uring_get_probe_ring()
 */
void io_uring_free_probe(struct io_uring_probe *probe);

static inline int io_uring_opcode_supported(const struct io_uring_probe *p, int op)
{
	if (op > p->last_op)
		return 0;
	return (p->ops[op].flags & IO_URING_OP_SUPPORTED) != 0;
}

#endif
fn c_int queue_init_params(u32 entries, Ring* ring, Params* p) @(cname="io_uring_queue_init_params");

fn c_int queue_init(u32 entries, Ring* ring, u32 flags) @(cname="io_uring_queue_init");

fn c_int queue_mmap(c_int fd, Params *p, Ring* ring) @(cname="io_uring_queue_mmap");
fn c_int Ring.dontfork(Ring *ring) @(cname="io_uring_ring_dontfork");
fn void Ring.exit(Ring* ring) @(cname="io_uring_queue_exit");

fn u32 peek_batch_cqe(Ring *ring, Cqe** cqes, u32 count) @(cname="io_uring_peek_batch_cqe");

#if 0
int io_uring_wait_cqes(struct io_uring *ring, struct io_uring_cqe **cqe_ptr,
		       unsigned wait_nr, struct __kernel_timespec *ts,
		       sigset_t *sigmask);
int io_uring_wait_cqe_timeout(struct io_uring *ring,
			      struct io_uring_cqe **cqe_ptr,
			      struct __kernel_timespec *ts);
#endif

fn c_int Ring.submit(Ring *ring) @(cname="io_uring_submit");
fn c_int Ring.submit_and_wait(Ring *ring, u32 wait_nr) @(cname="io_uring_submit_and_wait");
fn Sqe* Ring.get_sqe(Ring *ring) @(cname="io_uring_get_sqe");

fn c_int Ring.register_buffers(Ring* ring, const uio.Iovec* iovecs,
			      u32 nr_iovecs) @(cname="io_uring_register_buffers");
fn c_int Ring.register_buffers_tags(Ring* ring, const uio.Iovec* iovecs,
				   const u64* tags, u32 nr) @(cname="io_uring_register_buffers_tags");
fn c_int Ring.register_buffers_update_tag(Ring* ring,
					 u32 off,
					 const uio.Iovec* iovecs,
					 const u64* tags, u32 nr) @(cname="io_uring_register_buffers_update_tag");
fn c_int Ring.unregister_buffers(Ring* ring) @(cname="io_uring_unregister_buffers");

#if 0
int io_uring_register_files(struct io_uring *ring, const int *files,
			    unsigned nr_files);
int io_uring_register_files_tags(struct io_uring *ring, const int *files,
				 const __u64 *tags, unsigned nr);
int io_uring_register_files_update_tag(struct io_uring *ring, unsigned off,
				       const int *files, const __u64 *tags,
				       unsigned nr_files);

int io_uring_unregister_files(struct io_uring *ring);
int io_uring_register_files_update(struct io_uring *ring, unsigned off,
				   int *files, unsigned nr_files);
int io_uring_register_eventfd(struct io_uring *ring, int fd);
int io_uring_register_eventfd_async(struct io_uring *ring, int fd);
int io_uring_unregister_eventfd(struct io_uring *ring);
int io_uring_register_probe(struct io_uring *ring, struct io_uring_probe *p,
			    unsigned nr);
int io_uring_register_personality(struct io_uring *ring);
int io_uring_unregister_personality(struct io_uring *ring, int id);
int io_uring_register_restrictions(struct io_uring *ring,
				   struct io_uring_restriction *res,
				   unsigned int nr_res);
int io_uring_enable_rings(struct io_uring *ring);
int __io_uring_sqring_wait(struct io_uring *ring);
int io_uring_register_iowq_aff(struct io_uring *ring, size_t cpusz,
				const cpu_set_t *mask);
int io_uring_unregister_iowq_aff(struct io_uring *ring);
int io_uring_register_iowq_max_workers(struct io_uring *ring, unsigned int *values);
#endif

/*
 * Helper for the peek/wait single cqe functions. Exported because of that,
 * but probably shouldn't be used directly in an application.
 */
fn c_int Ring.get_cqe_internal(Ring *ring, Cqe** cqe_ptr, u32 submit, u32 wait_nr, csignal.Sigset* sigmask) @(cname="__io_uring_get_cqe");

#if 0
//define LIBURING_UDATA_TIMEOUT	((__u64) -1)

//define io_uring_for_each_cqe(ring, head, cqe)				\
	/*								\
	 * io_uring_smp_load_acquire() enforces the order of tail	\
	 * and CQE reads.						\
	 */								\
	for (head = *(ring)->cq.khead;					\
	     (cqe = (head != io_uring_smp_load_acquire((ring)->cq.ktail) ? \
		&(ring)->cq.cqes[head & (*(ring)->cq.kring_mask)] : NULL)); \
	     head++)							\

#endif
/*
 * Must be called after io_uring_for_each_cqe()
 */
fn void Ring.cq_advance(Ring *ring, u32 nr) @(cname="io_uring_cq_advance")
{
	if (nr) {
		Cq* cq = &ring.cq;

		/*
		 * Ensure that the kernel only sees the new value of the head
		 * index after the CQEs have been read.
		 */
        // TODO now dirty macro usage starts (ending up in the stdatomics mess)
		//io_uring_smp_store_release(cq.khead, *cq.khead + nr);
        *cq.khead = *cq.khead + nr;
	}
}

/*
 * Must be called after io_uring_{peek,wait}_cqe() after the cqe has
 * been processed by the application.
 */
fn void Ring.cqe_seen(Ring* ring, Cqe* cqe) @(cname="io_uring_cqe_seen")
{
	if (cqe) ring.cq_advance(1);
}

/*
 * Command prep helpers
 */
fn void Sqe.set_data(Sqe* sqe, void* data) @(cname="io_uring_sqe_set_data")
{
	sqe.user_data = data;
}

fn void *Cqe.get_data(const Cqe* cqe) @(cname="io_uring_sqe_get_data")
{
	//return (void *) (uintptr_t) cqe->user_data
	return cqe.user_data;
}

fn void Sqe.set_flags(Sqe* sqe, u32 flags) @(cname="io_uring_sqe_set_flags")
{
	sqe.flags = (u8)flags;
}
#if 0

static inline void __io_uring_set_target_fixed_file(struct io_uring_sqe *sqe,
						    unsigned int file_index)
{
	/* 0 means no fixed files, indexes should be encoded as "index + 1" */
	sqe->file_index = file_index + 1;
}
#endif

fn void prep_rw(c_int op, Sqe* sqe, c_int fd, const void* addr, u32 len, u64 offset) @(cname="io_uring_prep_rw")
{
	sqe.opcode = (u8)op;
	sqe.flags = 0;
	sqe.ioprio = 0;
	sqe.fd = fd;
	sqe.off = offset;
	sqe.addr = (u64)addr;
	sqe.len = len;
	sqe.rw_flags = 0;
	sqe.user_data = 0;
	sqe.buf_index = 0;
	sqe.personality = 0;
	sqe.file_index = 0;
	sqe.__pad2[0] = sqe.__pad2[1] = 0;
}
#if 0

/**
 * @pre Either fd_in or fd_out must be a pipe.
 * @param off_in If fd_in refers to a pipe, off_in must be (int64_t) -1;
 *               If fd_in does not refer to a pipe and off_in is (int64_t) -1, then bytes are read
 *               from fd_in starting from the file offset and it is adjust appropriately;
 *               If fd_in does not refer to a pipe and off_in is not (int64_t) -1, then the
 *               starting offset of fd_in will be off_in.
 * @param off_out The description of off_in also applied to off_out.
 * @param splice_flags see man splice(2) for description of flags.
 *
 * This splice operation can be used to implement sendfile by splicing to an intermediate pipe
 * first, then splice to the final destination.
 * In fact, the implementation of sendfile in kernel uses splice internally.
 *
 * NOTE that even if fd_in or fd_out refers to a pipe, the splice operation can still failed with
 * EINVAL if one of the fd doesn't explicitly support splice operation, e.g. reading from terminal
 * is unsupported from kernel 5.7 to 5.11.
 * Check issue 291 for more information.
 */
static inline void io_uring_prep_splice(struct io_uring_sqe *sqe,
					int fd_in, int64_t off_in,
					int fd_out, int64_t off_out,
					unsigned int nbytes,
					unsigned int splice_flags)
{
	io_uring_prep_rw(IORING_OP_SPLICE, sqe, fd_out, NULL, nbytes,
				(__u64) off_out);
	sqe->splice_off_in = (__u64) off_in;
	sqe->splice_fd_in = fd_in;
	sqe->splice_flags = splice_flags;
}

static inline void io_uring_prep_tee(struct io_uring_sqe *sqe,
				     int fd_in, int fd_out,
				     unsigned int nbytes,
				     unsigned int splice_flags)
{
	io_uring_prep_rw(IORING_OP_TEE, sqe, fd_out, NULL, nbytes, 0);
	sqe->splice_off_in = 0;
	sqe->splice_fd_in = fd_in;
	sqe->splice_flags = splice_flags;
}
#endif

fn void Sqe.prep_readv(Sqe* sqe, c_int fd, const uio.Iovec* iovecs, u32 nr_vecs, u64 offset) @(cname="io_uring_prep_readv")
{
	prep_rw(Op.READV, sqe, fd, iovecs, nr_vecs, offset);
}

fn void Sqe.prep_read_fixed(Sqe* sqe, c_int fd, void* buf, u32 nbytes, u64 offset, c_int buf_index) @(cname="io_uring_prep_read_fixed")
{
	prep_rw(Op.READ_FIXED, sqe, fd, buf, nbytes, offset);
	sqe.buf_index = (u16)buf_index;
}

fn void Sqe.prep_writev(Sqe* sqe, c_int fd, const uio.Iovec* iovecs, u32 nr_vecs, u64 offset) @(cname="io_uring_prep_writev")
{
	prep_rw(Op.WRITEV, sqe, fd, iovecs, nr_vecs, offset);
}

fn void Sqe.prep_write_fixed(Sqe* sqe, c_int fd, const void* buf, u32 nbytes, u64 offset, c_int buf_index) @(cname="io_uring_prep_write_fixed")
{
	prep_rw(Op.WRITE_FIXED, sqe, fd, buf, nbytes, offset);
	sqe.buf_index = (u16)buf_index;
}

#if 0
static inline void io_uring_prep_recvmsg(struct io_uring_sqe *sqe, int fd,
					 struct msghdr *msg, unsigned flags)
{
	io_uring_prep_rw(IORING_OP_RECVMSG, sqe, fd, msg, 1, 0);
	sqe->msg_flags = flags;
}

static inline void io_uring_prep_sendmsg(struct io_uring_sqe *sqe, int fd,
					 const struct msghdr *msg, unsigned flags)
{
	io_uring_prep_rw(IORING_OP_SENDMSG, sqe, fd, msg, 1, 0);
	sqe->msg_flags = flags;
}

static inline unsigned __io_uring_prep_poll_mask(unsigned poll_mask)
{
#if __BYTE_ORDER == __BIG_ENDIAN
	poll_mask = __swahw32(poll_mask);
#endif
	return poll_mask;
}

static inline void io_uring_prep_poll_add(struct io_uring_sqe *sqe, int fd,
					  unsigned poll_mask)
{
	io_uring_prep_rw(IORING_OP_POLL_ADD, sqe, fd, NULL, 0, 0);
	sqe->poll32_events = __io_uring_prep_poll_mask(poll_mask);
}

static inline void io_uring_prep_poll_multishot(struct io_uring_sqe *sqe,
						int fd, unsigned poll_mask)
{
	io_uring_prep_poll_add(sqe, fd, poll_mask);
	sqe->len = IORING_POLL_ADD_MULTI;
}

static inline void io_uring_prep_poll_remove(struct io_uring_sqe *sqe,
					     void *user_data)
{
	io_uring_prep_rw(IORING_OP_POLL_REMOVE, sqe, -1, user_data, 0, 0);
}

static inline void io_uring_prep_poll_update(struct io_uring_sqe *sqe,
					     void *old_user_data,
					     void *new_user_data,
					     unsigned poll_mask, unsigned flags)
{
	io_uring_prep_rw(IORING_OP_POLL_REMOVE, sqe, -1, old_user_data, flags,
			 (__u64)(uintptr_t)new_user_data);
	sqe->poll32_events = __io_uring_prep_poll_mask(poll_mask);
}

static inline void io_uring_prep_fsync(struct io_uring_sqe *sqe, int fd,
				       unsigned fsync_flags)
{
	io_uring_prep_rw(IORING_OP_FSYNC, sqe, fd, NULL, 0, 0);
	sqe->fsync_flags = fsync_flags;
}
#endif

fn void Sqe.prep_nop(Sqe* sqe) @(cname="io_uring_prep_nop")
{
	prep_rw(Op.NOP, sqe, -1, nil, 0, 0);
}

fn void Sqe.prep_timeout(Sqe* sqe, linux_types.Timespec* ts,
					 u32 count, u32 flags) @(cname="io_uring_prep_timeout")
{
	prep_rw(Op.TIMEOUT, sqe, -1, ts, 1, count);
	sqe.timeout_flags = flags;
}

fn void Sqe.prep_timeout_remove(Sqe* sqe, u64 user_data, u32 flags) @(cname="io_uring_prep_timeout_remove")
{
	//prep_rw(Op.TIMEOUT_REMOVE, sqe, -1, (void *)(unsigned long)user_data, 0, 0);
	prep_rw(Op.TIMEOUT_REMOVE, sqe, -1, user_data, 0, 0);
	sqe.timeout_flags = flags;
}

fn void Sqe.prep_timeout_update(Sqe* sqe, linux_types.Timespec* ts, u64 user_data, u32 flags) @(cname="io_uring_prep_timeout_update")
{
	prep_rw(Op.TIMEOUT_REMOVE, sqe, -1, (void*)user_data, 0, (usize)ts);
	sqe.timeout_flags = flags | TIMEOUT_UPDATE;
}

fn void Sqe.prep_accept(Sqe* sqe, c_int fd,
                        sys_socket.Sockaddr* addr,
                        u32* addrlen,
                        c_int flags) @(cname="io_uring_prep_accept")
{
	prep_rw(Op.ACCEPT, sqe, fd, addr, 0, (u64)addrlen);
	sqe.accept_flags = (u32)flags;
}

#if 0
/* accept directly into the fixed file table */
static inline void io_uring_prep_accept_direct(struct io_uring_sqe *sqe, int fd,
					       struct sockaddr *addr,
					       socklen_t *addrlen, int flags,
					       unsigned int file_index)
{
	io_uring_prep_accept(sqe, fd, addr, addrlen, flags);
	__io_uring_set_target_fixed_file(sqe, file_index);
}
#endif

fn void Sqe.prep_cancel(Sqe* sqe, void* user_data, c_int flags) @(cname="io_uring_prep_cancel")
{
	prep_rw(Op.ASYNC_CANCEL, sqe, -1, user_data, 0, 0);
	sqe.cancel_flags = (u32)flags;
}

fn void Sqe.prep_link_timeout(Sqe* sqe, linux_types.Timespec* ts,
					      u32 flags) @(cname="io_uring_prep_link_timeout")
{
	prep_rw(Op.LINK_TIMEOUT, sqe, -1, ts, 1, 0);
	sqe.timeout_flags = flags;
}

#if 0
static inline void io_uring_prep_connect(struct io_uring_sqe *sqe, int fd,
					 const struct sockaddr *addr,
					 socklen_t addrlen)
{
	io_uring_prep_rw(IORING_OP_CONNECT, sqe, fd, addr, 0, addrlen);
}

static inline void io_uring_prep_files_update(struct io_uring_sqe *sqe,
					      int *fds, unsigned nr_fds,
					      int offset)
{
	io_uring_prep_rw(IORING_OP_FILES_UPDATE, sqe, -1, fds, nr_fds,
				(__u64) offset);
}

static inline void io_uring_prep_fallocate(struct io_uring_sqe *sqe, int fd,
					   int mode, off_t offset, off_t len)
{

	io_uring_prep_rw(IORING_OP_FALLOCATE, sqe, fd,
			(const uintptr_t *) (unsigned long) len,
			(unsigned int) mode, (__u64) offset);
}

static inline void io_uring_prep_openat(struct io_uring_sqe *sqe, int dfd,
					const char *path, int flags, mode_t mode)
{
	io_uring_prep_rw(IORING_OP_OPENAT, sqe, dfd, path, mode, 0);
	sqe->open_flags = (__u32) flags;
}

/* open directly into the fixed file table */
static inline void io_uring_prep_openat_direct(struct io_uring_sqe *sqe,
					       int dfd, const char *path,
					       int flags, mode_t mode,
					       unsigned file_index)
{
	io_uring_prep_openat(sqe, dfd, path, flags, mode);
	__io_uring_set_target_fixed_file(sqe, file_index);
}

#endif

fn void Sqe.prep_close(Sqe* sqe, c_int fd) @(cname="io_uring_prep_close")
{
	prep_rw(Op.CLOSE, sqe, fd, nil, 0, 0);
}

fn void Sqe.prep_read(Sqe* sqe, c_int fd, void* buf, u32 nbytes, u64 offset) @(cname="io_uring_prep_read")
{
	prep_rw(Op.READ, sqe, fd, buf, nbytes, offset);
}

fn void Sqe.prep_write(Sqe* sqe, c_int fd, const void* buf, u32 nbytes, u64 offset) @(cname="io_uring_prep_write")
{
	prep_rw(Op.WRITE, sqe, fd, buf, nbytes, offset);
}

#if 0
struct statx;
static inline void io_uring_prep_statx(struct io_uring_sqe *sqe, int dfd,
				const char *path, int flags, unsigned mask,
				struct statx *statxbuf)
{
	io_uring_prep_rw(IORING_OP_STATX, sqe, dfd, path, mask,
				(__u64) (unsigned long) statxbuf);
	sqe->statx_flags = (__u32) flags;
}

static inline void io_uring_prep_fadvise(struct io_uring_sqe *sqe, int fd,
					 __u64 offset, off_t len, int advice)
{
	io_uring_prep_rw(IORING_OP_FADVISE, sqe, fd, NULL, (__u32) len, offset);
	sqe->fadvise_advice = (__u32) advice;
}

static inline void io_uring_prep_madvise(struct io_uring_sqe *sqe, void *addr,
					 off_t length, int advice)
{
	io_uring_prep_rw(IORING_OP_MADVISE, sqe, -1, addr, (__u32) length, 0);
	sqe->fadvise_advice = (__u32) advice;
}

static inline void io_uring_prep_send(struct io_uring_sqe *sqe, int sockfd,
				      const void *buf, size_t len, int flags)
{
	io_uring_prep_rw(IORING_OP_SEND, sqe, sockfd, buf, (__u32) len, 0);
	sqe->msg_flags = (__u32) flags;
}

static inline void io_uring_prep_recv(struct io_uring_sqe *sqe, int sockfd,
				      void *buf, size_t len, int flags)
{
	io_uring_prep_rw(IORING_OP_RECV, sqe, sockfd, buf, (__u32) len, 0);
	sqe->msg_flags = (__u32) flags;
}

static inline void io_uring_prep_openat2(struct io_uring_sqe *sqe, int dfd,
					const char *path, struct open_how *how)
{
	io_uring_prep_rw(IORING_OP_OPENAT2, sqe, dfd, path, sizeof(*how),
				(uint64_t) (uintptr_t) how);
}

/* open directly into the fixed file table */
static inline void io_uring_prep_openat2_direct(struct io_uring_sqe *sqe,
						int dfd, const char *path,
						struct open_how *how,
						unsigned file_index)
{
	io_uring_prep_openat2(sqe, dfd, path, how);
	__io_uring_set_target_fixed_file(sqe, file_index);
}

struct epoll_event;
static inline void io_uring_prep_epoll_ctl(struct io_uring_sqe *sqe, int epfd,
					   int fd, int op,
					   struct epoll_event *ev)
{
	io_uring_prep_rw(IORING_OP_EPOLL_CTL, sqe, epfd, ev,
				(__u32) op, (__u32) fd);
}

static inline void io_uring_prep_provide_buffers(struct io_uring_sqe *sqe,
						 void *addr, int len, int nr,
						 int bgid, int bid)
{
	io_uring_prep_rw(IORING_OP_PROVIDE_BUFFERS, sqe, nr, addr, (__u32) len,
				(__u64) bid);
	sqe->buf_group = (__u16) bgid;
}

static inline void io_uring_prep_remove_buffers(struct io_uring_sqe *sqe,
						int nr, int bgid)
{
	io_uring_prep_rw(IORING_OP_REMOVE_BUFFERS, sqe, nr, NULL, 0, 0);
	sqe->buf_group = (__u16) bgid;
}

static inline void io_uring_prep_shutdown(struct io_uring_sqe *sqe, int fd,
					  int how)
{
	io_uring_prep_rw(IORING_OP_SHUTDOWN, sqe, fd, NULL, (__u32) how, 0);
}

static inline void io_uring_prep_unlinkat(struct io_uring_sqe *sqe, int dfd,
					  const char *path, int flags)
{
	io_uring_prep_rw(IORING_OP_UNLINKAT, sqe, dfd, path, 0, 0);
	sqe->unlink_flags = (__u32) flags;
}

static inline void io_uring_prep_renameat(struct io_uring_sqe *sqe, int olddfd,
					  const char *oldpath, int newdfd,
					  const char *newpath, int flags)
{
	io_uring_prep_rw(IORING_OP_RENAMEAT, sqe, olddfd, oldpath, (__u32) newdfd,
				(uint64_t) (uintptr_t) newpath);
	sqe->rename_flags = (__u32) flags;
}

static inline void io_uring_prep_sync_file_range(struct io_uring_sqe *sqe,
						 int fd, unsigned len,
						 __u64 offset, int flags)
{
	io_uring_prep_rw(IORING_OP_SYNC_FILE_RANGE, sqe, fd, NULL, len, offset);
	sqe->sync_range_flags = (__u32) flags;
}

static inline void io_uring_prep_mkdirat(struct io_uring_sqe *sqe, int dfd,
					const char *path, mode_t mode)
{
	io_uring_prep_rw(IORING_OP_MKDIRAT, sqe, dfd, path, mode, 0);
}

static inline void io_uring_prep_symlinkat(struct io_uring_sqe *sqe,
					const char *target, int newdirfd, const char *linkpath)
{
	io_uring_prep_rw(IORING_OP_SYMLINKAT, sqe, newdirfd, target, 0,
				(uint64_t) (uintptr_t) linkpath);
}

static inline void io_uring_prep_linkat(struct io_uring_sqe *sqe, int olddfd,
					const char *oldpath, int newdfd,
					const char *newpath, int flags)
{
	io_uring_prep_rw(IORING_OP_LINKAT, sqe, olddfd, oldpath, (__u32) newdfd,
				(uint64_t) (uintptr_t) newpath);
	sqe->hardlink_flags = (__u32) flags;
}
#endif

/*
 * Returns number of unconsumed (if SQPOLL) or unsubmitted entries exist in
 * the SQ ring
 */
fn u32 io_uring_sq_ready(const Ring* ring)
{
	/*
	 * Without a barrier, we could miss an update and think the SQ wasn't ready.
	 * We don't need the load acquire for non-SQPOLL since then we drive updates.
	 */
	if (ring.flags & SETUP_SQPOLL)
		//return ring.sq.sqe_tail - io_uring_smp_load_acquire(ring.sq.khead);
		return ring.sq.sqe_tail - *ring.sq.khead;

	/* always use real head, to avoid losing sync for short submit */
	return ring.sq.sqe_tail - *ring.sq.khead;
}

/*
 * Returns how much space is left in the SQ ring.
 */
fn u32 sq_space_left(const Ring* ring)
{
	return *ring.sq.kring_entries - io_uring_sq_ready(ring);
}

#if 0
/*
 * Only applicable when using SQPOLL - allows the caller to wait for space
 * to free up in the SQ ring, which happens when the kernel side thread has
 * consumed one or more entries. If the SQ ring is currently non-full, no
 * action is taken. Note: may return -EINVAL if the kernel doesn't support
 * this feature.
 */
static inline int io_uring_sqring_wait(struct io_uring *ring)
{
	if (!(ring->flags & IORING_SETUP_SQPOLL))
		return 0;
	if (io_uring_sq_space_left(ring))
		return 0;

	return __io_uring_sqring_wait(ring);
}

/*
 * Returns how many unconsumed entries are ready in the CQ ring
 */
static inline unsigned io_uring_cq_ready(const struct io_uring *ring)
{
	return io_uring_smp_load_acquire(ring->cq.ktail) - *ring->cq.khead;
}

/*
 * Returns true if the eventfd notification is currently enabled
 */
static inline bool io_uring_cq_eventfd_enabled(const struct io_uring *ring)
{
	if (!ring->cq.kflags)
		return true;

	return !(*ring->cq.kflags & IORING_CQ_EVENTFD_DISABLED);
}

/*
 * Toggle eventfd notification on or off, if an eventfd is registered with
 * the ring.
 */
static inline int io_uring_cq_eventfd_toggle(struct io_uring *ring,
					     bool enabled)
{
	uint32_t flags;

	if (!!enabled == io_uring_cq_eventfd_enabled(ring))
		return 0;

	if (!ring->cq.kflags)
		return -EOPNOTSUPP;

	flags = *ring->cq.kflags;

	if (enabled)
		flags &= ~IORING_CQ_EVENTFD_DISABLED;
	else
		flags |= IORING_CQ_EVENTFD_DISABLED;

	IO_URING_WRITE_ONCE(*ring->cq.kflags, flags);

	return 0;
}
#endif

/*
 * Return an IO completion, waiting for 'wait_nr' completions if one isn't
 * readily available. Returns 0 with cqe_ptr filled in on success, -errno on
 * failure.
 */
fn c_int Ring.wait_cqe_nr(Ring *ring, Cqe** cqe_ptr, u32 wait_nr) @(cname="io_uring_wait_cqe_nr")
{
	return Ring.get_cqe_internal(ring, cqe_ptr, 0, wait_nr, nil);
}

/*
 * Return an IO completion, if one is readily available. Returns 0 with
 * cqe_ptr filled in on success, -errno on failure.
 */
fn c_int Ring.peek_cqe(Ring* ring, Cqe** cqe_ptr) @(cname="io_uring_peek_cqe")
{
	return ring.wait_cqe_nr(cqe_ptr, 0);
}

/*
 * Return an IO completion, waiting for it if necessary. Returns 0 with
 * cqe_ptr filled in on success, -errno on failure.
 */
fn c_int Ring.wait_cqe(Ring* ring, Cqe **cqe_ptr) @(cname="io_uring_wait_cqe")
{
	return ring.wait_cqe_nr(cqe_ptr, 1);
}

fn isize mlock_size(u32 entries, u32 flags) @(cname="io_uring_mlock_size");
fn isize mlock_size_params(u32 entries, Params *p) @(cname="io_uring_mlock_size_params");

// ------ liburing/barrier.h ------

// This is one big mess of macros and defines..nasty

// ------ liburing/io_uring.h ------

/*
 * io_uring_setup() flags
 */
const u32 SETUP_IOPOLL     = (1 << 0);   /* io_context is polled */
const u32 SETUP_SQPOLL     = (1 << 1);   /* SQ poll thread */
const u32 SETUP_SQ_AFF     = (1 << 2);   /* sq_thread_cpu is valid */
const u32 SETUP_CQSIZE     = (1 << 3);   /* app defines CQ size */
const u32 SETUP_CLAMP      = (1 << 4);   /* clamp SQ/CQ ring sizes */
const u32 SETUP_ATTACH_WQ  = (1 << 5);   /* attach to existing wq */
const u32 SETUP_R_DISABLED = (1 << 6);   /* start with ring disabled */


type Op enum u8 {
    NOP,
    READV,
    WRITEV,
    FSYNC,
    READ_FIXED,
    WRITE_FIXED,
    POLL_ADD,
    POLL_REMOVE,
    SYNC_FILE_RANGE,
    SENDMSG,
    RECVMSG,
    TIMEOUT,
    TIMEOUT_REMOVE,
    ACCEPT,
    ASYNC_CANCEL,
    LINK_TIMEOUT,
    CONNECT,
    FALLOCATE,
    OPENAT,
    CLOSE,
    FILES_UPDATE,
    STATX,
    READ,
    WRITE,
    FADVISE,
    MADVISE,
    SEND,
    RECV,
    OPENAT2,
    EPOLL_CTL,
    SPLICE,
    PROVIDE_BUFFERS,
    REMOVE_BUFFERS,
    TEE,
    SHUTDOWN,
    RENAMEAT,
    UNLINKAT,
    MKDIRAT,
    SYMLINKAT,
    LINKAT,

    /* this goes last, obviously */
    //IORING_OP_LAST,
}


type Sqe struct @(cname="io_uring_sqe") {
    u8    opcode;     /* type of operation for this sqe */
    u8    flags;      /* IOSQE_ flags */
    u16   ioprio;     /* ioprio for the request */
    i32   fd;     /* file descriptor to do IO on */
    union {
        u64   off;    /* offset into file */
        u64   addr2;
    }
    union {
        u64   addr;   /* pointer to buffer or iovecs */
        u64   splice_off_in;
    }
    u32   len;        /* buffer size or number of iovecs */
    union {
        Kernel_rwf_t  rw_flags; // __kernel_rwf_t
        u32       fsync_flags;
        u16       poll_events;    /* compatibility */
        u32       poll32_events;  /* word-reversed for BE */
        u32       sync_range_flags;
        u32       msg_flags;
        u32       timeout_flags;
        u32       accept_flags;
        u32       cancel_flags;
        u32       open_flags;
        u32       statx_flags;
        u32       fadvise_advice;
        u32       splice_flags;
        u32       rename_flags;
        u32       unlink_flags;
        u32       hardlink_flags;
    }
    u64   user_data;  /* data to be passed back at completion time */
    /* pack this to avoid bogus arm OABI complaints */
    union {
        /* index into fixed buffers, if used */
        u16   buf_index;
        /* for grouped buffer selection */
        u16   buf_group;
    }
    /* personality to use, if used */
    u16   personality;
    union {
        i32   splice_fd_in;
        u32   file_index;
    }
    u64[2]   __pad2;
}
static_assert(64, sizeof(Sqe));

type Cqe struct @(cname="io_uring_cqe") {
    u64   user_data;  /* sqe->data submission passed back */
    i32   res;        /* result code for this event */
    u32   flags;

}
static_assert(16, sizeof(Cqe));

/*
 * cqe->flags
 *
 * IORING_CQE_F_BUFFER  If set, the upper 16 bits are the buffer ID
 * IORING_CQE_F_MORE    If set, parent SQE will generate more CQE entries
 */
const u32 CQE_F_BUFFER = (1 << 0);
const u32 CQE_F_MORE   = (1 << 1);


//enum {
//    IORING_CQE_BUFFER_SHIFT     = 16,
//};

/*
 * Magic offsets for the application to mmap the data it needs
 */
const u64 OFF_SQ_RING = 0;
const u64 OFF_CQ_RING = 0x8000000;
const u64 OFF_SQES    = 0x10000000;

/*
 * Filled with the offset for mmap(2)
 */
type SqringOffsets struct @(cname="io_sqring_offsets") {
    u32 head;
    u32 tail;
    u32 ring_mask;
    u32 ring_entries;
    u32 flags;
    u32 dropped;
    u32 array;
    u32 resv1;
    u64 resv2;
}
static_assert(40, sizeof(SqringOffsets));

// TODO rename
// sq_ring->flags
const u32 SQ_NEED_WAKEUP = (1 << 0); /* needs io_uring_enter wakeup */
const u32 SQ_CQ_OVERFLOW = (1 << 1); /* CQ ring is overflown */


type CqringOffsets struct @(cname="io_cqring_offsets") {
    u32 head;
    u32 tail;
    u32 ring_mask;
    u32 ring_entries;
    u32 overflow;
    u32 cqes;
    u32 flags;
    u32 resv1;
    u64 resv2;
}
static_assert(40, sizeof(CqringOffsets));

type Params struct @(cname="io_uring_params") {
    u32 sq_entries;
    u32 cq_entries;
    u32 flags;
    u32 sq_thread_cpu;
    u32 sq_thread_idle;
    u32 features;
    u32 wq_fd;
    u32[3] resv;
    SqringOffsets sq_off;
    CqringOffsets cq_off;
}
static_assert(120, sizeof(Params));

/*
 * sqe->timeout_flags
 */
const u32 TIMEOUT_ABS         = (1 << 0);
const u32 TIMEOUT_UPDATE      = (1 << 1);
const u32 TIMEOUT_BOOTTIME    = (1 << 2);
const u32 TIMEOUT_REALTIME    = (1 << 3);
const u32 LINK_TIMEOUT_UPDATE = (1 << 4);
const u32 TIMEOUT_CLOCK_MASK  = TIMEOUT_BOOTTIME | TIMEOUT_REALTIME;
const u32 TIMEOUT_UPDATE_MASK = TIMEOUT_UPDATE | LINK_TIMEOUT_UPDATE;



// TODO move this to some other lib/module
// ------ liburing/barrier.h ------
//#define io_uring_smp_store_release(p, v)            \
//    atomic_store_explicit((_Atomic __typeof__(*(p)) *)(p), (v), \
//                  memory_order_release)

// ------ linux/fs.h ------

type Kernel_rwf_t c_int;


